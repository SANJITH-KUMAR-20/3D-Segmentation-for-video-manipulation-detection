{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TE8IDGLbsXw",
    "outputId": "d7b383cb-c71f-4cad-abd1-5b3b35972eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: av in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (11.0.0)\n",
      "Requirement already satisfied: facenet_pytorch in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from facenet_pytorch) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from facenet_pytorch) (2.31.0)\n",
      "Requirement already satisfied: torchvision in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from facenet_pytorch) (0.17.1)\n",
      "Requirement already satisfied: pillow in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from facenet_pytorch) (10.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from requests->facenet_pytorch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from requests->facenet_pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from requests->facenet_pytorch) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from requests->facenet_pytorch) (2024.2.2)\n",
      "Requirement already satisfied: torch in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from torchvision->facenet_pytorch) (2.2.1)\n",
      "Requirement already satisfied: filelock in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from torch->torchvision->facenet_pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from torch->torchvision->facenet_pytorch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from torch->torchvision->facenet_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from torch->torchvision->facenet_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from torch->torchvision->facenet_pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from torch->torchvision->facenet_pytorch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from jinja2->torch->torchvision->facenet_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from sympy->torch->torchvision->facenet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: ffmpeg-python in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: future in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from matplotlib) (6.1.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/praveenbenedict/miniconda3/envs/deepcheck-faceswap-video-seg/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install av\n",
    "!pip install facenet_pytorch\n",
    "!pip install ffmpeg-python\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eW__GbyMeZAU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\Phosphene.AI\\sizeinvar_timesformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import numpy as np\n",
    "import facenet_pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg\n",
    "import torchvision.io as tio\n",
    "import av\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4qhbsUY7eZDM"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "#zip_file = \"/content/drive/MyDrive/Phosphene/sample_data.zip\"\n",
    "#with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "#        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4Cj25WkQexZE"
   },
   "outputs": [],
   "source": [
    "train_dir = \"sample_data/target/train\"\n",
    "val_dir = \"sample_data/target/val\"\n",
    "train_mask_dir = \"sample_data/target_mask/train\"\n",
    "val_mask_dir = \"sample_data/target_mask/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G4F8jVnaexrk"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BgvqLluUeZJE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, directory, target_dir, transform=None, frame_rate=24, pick_only=24):\n",
    "        self.directory = directory\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.frame_rate = frame_rate\n",
    "        self.pick_only = pick_only\n",
    "\n",
    "        self.videos = sorted(os.listdir(self.directory))\n",
    "        self.masks = sorted(os.listdir(self.target_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.directory, self.videos[idx])\n",
    "        mask_path = os.path.join(self.target_dir, self.masks[idx])\n",
    "    \n",
    "        video_frames, mask_frames = self.extract_frames(video_path, mask_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            video_frames = [self.transform(frame) for frame in video_frames]\n",
    "            mask_frames = [self.transform(frame) for frame in mask_frames]\n",
    "\n",
    "        video_frames = torch.stack(video_frames).permute(1, 0, 2, 3)\n",
    "        mask_frames = torch.stack(mask_frames).permute(1, 0, 2, 3)\n",
    "\n",
    "        return video_frames, mask_frames\n",
    "\n",
    "    def extract_frames(self, video_path, mask_path):\n",
    "        cap_video = cv2.VideoCapture(video_path)\n",
    "        cap_mask = cv2.VideoCapture(mask_path)\n",
    "        frame_count = int(cap_video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        desired_frame_count = self.pick_only\n",
    "        \n",
    "        # Pick a random starting point within the video \n",
    "        start_idx = random.randint(0, max(0, frame_count - desired_frame_count))\n",
    "        \n",
    "        video_frames = []\n",
    "        mask_frames = []\n",
    "        \n",
    "        cap_video.set(cv2.CAP_PROP_POS_FRAMES, start_idx)  \n",
    "        cap_mask.set(cv2.CAP_PROP_POS_FRAMES, start_idx)  \n",
    "        \n",
    "        for i in range(desired_frame_count):\n",
    "            ret_video, frame_video = cap_video.read()\n",
    "            ret_mask, frame_mask = cap_mask.read()\n",
    "            if not (ret_video and ret_mask):\n",
    "                break\n",
    "            frame_video = cv2.cvtColor(frame_video, cv2.COLOR_BGR2RGB)\n",
    "            frame_mask = cv2.cvtColor(frame_mask, cv2.COLOR_BGR2GRAY)\n",
    "            video_frames.append(frame_video)\n",
    "            \n",
    "            mask_frames.append(frame_mask)\n",
    "           \n",
    "        \n",
    "        cap_video.release()\n",
    "        cap_mask.release()\n",
    "        return video_frames, mask_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rm4U0K_2eZL9"
   },
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(train_dir, target_dir=train_mask_dir, transform=transform)\n",
    "val_dataset = VideoDataset(val_dir, target_dir=val_mask_dir,transform=transform)\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmgRvtsjeZO7",
    "outputId": "d88f66ec-8fc4-4fa9-d46d-19202df99e34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWgEBEpReZR0",
    "outputId": "669cc75e-07d2-4f11-8c1a-f32783e078bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video: sample_data/target/train\\train_00000409.mp4\n",
      "Loading mask: sample_data/target_mask/train\\train_00000409_mask.mp4\n",
      "starting index: 52\n",
      "Video batch shape: torch.Size([1, 3, 24, 224, 224])\n",
      "Mask batch shape: torch.Size([1, 1, 24, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for videos, masks in train_loader:\n",
    "    print(\"Video batch shape:\", videos.shape)\n",
    "    print(\"Mask batch shape:\", masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9e1OJTkeZUr",
    "outputId": "3f0ed097-2787-4015-d58b-18e69fa13019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 102)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "67tM767LiB0X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(64, 1, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x1)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = UNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3WfVuNdiqwZ",
    "outputId": "9faf6a8b-457a-449d-bcdf-fb87a73b73c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): ConvTranspose3d(64, 1, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NodXVbw2jDu3"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEnSzbAUkPO9",
    "outputId": "7cf7e58c-4b44-4f46-9f76-ce4eb5d53b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.UNet'>\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'decoder', 'double', 'dump_patches', 'encoder', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true,y_pred):\n",
    "    smooth = 1e-6\n",
    "    intersection = torch.sum(y_true*y_pred)\n",
    "    union = torch.sum(y_true) + torch.sum(y_pred)\n",
    "    dice = (2. * intersection + smooth)/(union+smooth)\n",
    "    return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIjiT5SMjb8O",
    "outputId": "827e717f-c85d-474a-e5d7-cc0278eeeade"
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    similarity_index = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (videos, masks) in enumerate(train_loader):\n",
    "        print(f\"Epochs : {epoch+1}\")\n",
    "        videos, masks = videos.to(device), masks.to(device)\n",
    "        print(f\"Batch {batch_idx+1}/{len(train_loader)}\")\n",
    "        print(f\"Video shape: {videos.shape}\")\n",
    "        print(f\"Mask shape: {masks.shape}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        print(f\"Output shape: {outputs.shape}\")\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        print(f\"Loss for batch {batch_idx+1}: {loss.item():.4f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        batch_size = videos.size(0)\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        #similarity index \n",
    "        outputs_binary = torch.sigmoid(outputs) > 0.5\n",
    "        similarity_index += dice_coefficient(masks, outputs_binary).item() * batch_size\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_similarity_index = similarity_index / total_samples\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{5}], Average Loss: {avg_loss:.4f}, Similarity Index: {avg_similarity_index:.4f}')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn-ot7YfpCaG",
    "outputId": "68b2bbdf-fbc1-4e94-d7cc-4d6ff65db504"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_iou = 0.0\n",
    "total_val_samples = 0\n",
    "\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (videos, masks) in enumerate(val_loader):\n",
    "        videos, masks = videos.to(device), masks.to(device)\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, masks)\n",
    "        val_loss += loss.item()\n",
    "            \n",
    "        \n",
    "            \n",
    "        total_val_samples += videos.size(0)\n",
    "    \n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "print(f'Validation Loss: {avg_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = epoch\n",
    "PATH = \"model.pt\"\n",
    "LOSS = val_loss\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "vXUEA2FRjXeM",
    "outputId": "b180bd0e-c1ce-435c-89d9-aca67f38d8cc"
   },
   "outputs": [],
   "source": [
    "for videos, masks in train_loader:\n",
    "      videos, masks = videos.to(device), masks.to(device)\n",
    "      break\n",
    "plt.imshow(transforms.ToPILImage()(outputs[0,:,3,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "KzdOuVzPjXhU",
    "outputId": "cad4fa8d-1582-4497-c0ec-6f79d5c6fdd7"
   },
   "outputs": [],
   "source": [
    "vid = outputs.cpu().permute(0,2,3,4,1).numpy()[0]\n",
    "plt.imshow(vid[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Uar5YYHpKr-",
    "outputId": "a7e4fb68-e124-4a04-96a3-abb3b128d056"
   },
   "outputs": [],
   "source": [
    "outputs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjITMBDDpK3N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
